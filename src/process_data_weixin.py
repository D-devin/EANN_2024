# encoding=utf-8
import pickle as pickle
import random
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import os
from collections import defaultdict
import sys, re
import pandas as pd
import math
from types import *
from gensim.models import Word2Vec
import jieba
from sklearn.cluster import AgglomerativeClustering
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import os.path
import gensim
import cv2

os.chdir(os.path.dirname(os.path.abspath(__file__)))
## æ¸…æ´—ï¼ˆæ´—å»æ ‡ç‚¹ï¼Œåˆ†è¯ï¼Œå»æ‰åœç”¨è¯ï¼‰ï¼Œå›¾ç‰‡è½¬åŒ–ï¼Œæ–‡æœ¬ä¸å›¾åƒåŒ¹é…ï¼Œåˆå¹¶æ–‡æœ¬è·å–è¯é¢‘ç‡ï¼Œè·å–è¯å‘é‡å°†è¿™æ¬¡æ•°æ®é›†çš„è¯å‘é‡åŠ å…¥åˆ°å…¶ä¸­
## æœ€åè¿”å›æ•°æ®é›†çš„dataï¼Œå°†æ–°çš„è¯å‘é‡ä¿å­˜ã€‚
def stopwords(path=r'E:\fakenews\EANN_2024\Data\weixin\cn_stopwords.txt'):
    stopwords = []
    for line in open(path, 'r').readlines():
        stopwords.append(line.strip())
    return stopwords
  
def ssl_clean(string):
    if isinstance(string, float):
        #print(string)
        string = "æ— "
        return string

    # å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¦æ¸…é™¤çš„ç‰¹æ®Šå­—ç¬¦çš„å­—ç¬¦ç±»
    special_chars = u"[ï¼Œã€‚ï¼šï¼›.`|â€œâ€â€”â€”_/&;@ã€?ï¼Ÿ''â€™â€˜â€œâ€ã€Šã€‹ï½ï¼ˆï¼‰#ï¼%ã€ã€‘:,+(ï½œ)ä¸¨â—â–¶â€¦! âœ…ãŠ™Òˆâ†‘â†“ğŸ¤£\s*nbsp-]"
    # ä½¿ç”¨re.sub()å‡½æ•°å’Œå­—ç¬¦ç±»æ¥æ¸…é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦
    string = re.sub(special_chars, "", string)
    # å»é™¤å­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦ï¼ˆæ³¨æ„ï¼šè¿™é‡Œä¸å¤„ç†ä¸­é—´çš„ç©ºç™½å­—ç¬¦ï¼‰
    return string.strip()


def update_image_url(output_df, images_dir):
    # åˆ›å»º DataFrame çš„å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    for index, row in output_df.iterrows():
        id_value = row['id']
        # æ„é€ ç›¸å¯¹è·¯å¾„
        relative_image_path = os.path.join(os.path.basename(images_dir), f'{id_value}.png')
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        image_file = os.path.join(images_dir, f'{id_value}.png')
        if not os.path.exists(image_file):
            print(f"Warning: Image file for id {id_value} does not exist at {image_file}")
            relative_image_path = 'images/missing.png'  # å ä½ç¬¦å›¾ç‰‡è·¯å¾„

        # æ›´æ–° DataFrame ä¸­çš„å›¾ç‰‡è·¯å¾„
        output_df.at[index, 'Image Url'] = relative_image_path

    return output_df

def check_content_for_errors(text):
    # å®šä¹‰æ— æ³•è¯»å–çš„å…³é”®è¯åˆ—è¡¨
    error_keywords = [
        "è´¦å· å±è”½", "å†…å®¹ æ— æ³• æŸ¥çœ‹", "å†…å®¹å‘å¸ƒè€… åˆ é™¤", "å¾®ä¿¡ å…¬ä¼— å¹³å° è¿è¥ ä¸­å¿ƒ",
        "è´¦å· è¿ç§»", "å…¬ä¼—å· ç¯å¢ƒå¼‚å¸¸"
    ]
    # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä»»ä½•å…³é”®è¯
    for keyword in error_keywords:
        if keyword in text:
            return "1"
    return "2"

def images_process (image_url):
    image_list = []
    for path in image_url:
        image_name = os.path.splitext(path)[0]
        data_transforms = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], 
                                 [0.229, 0.224, 0.225])])
        try:
            im = cv2.imread(image_name)
            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
            im = data_transforms(im)
            image_list.append(im) 
        except:
            print("err_image_name:"+image_name)
            print("image_path:"+path)
    print("image_length:"+str(len(image_list)))
    return image_list

def get_w(model, word_text,index, k):
        # vocab_size = len(word_vecs)
    word_index_map= dict()
    W = np.zeros(shape=(len(index) + 1, k), dtype='float32')
    W[0] = np.zeros(k, dtype='float32')
    i = 1
    for word in word_text:
        W[i] = model[word]
        word_index_map[word] = i
        i += 1
    return W,word_index_map

def text_to_word2vec(save_path,all_text,min_df = 5,vector_size = 100):
    """
    å°†è¯è¡¨è½¬åŒ–ä¸ºè¯å‘é‡
    ç„¶åä¿å­˜è¯å‘é‡è¡¨ä¸ºæ–‡ä»¶
    é‡‡ç”¨gensimè¯»å–
    åŠ å…¥çš„æ–°è¯è¡¨çš„æœ€ä½é¢‘ç‡ä¸º1
    min_countæ˜¯æœ€ä½å‡ºç°æ•°ï¼Œé»˜è®¤æ•°å€¼æ˜¯5ï¼›
    sizeæ˜¯gensim Word2Vecå°†è¯æ±‡æ˜ å°„åˆ°çš„Nç»´ç©ºé—´çš„ç»´åº¦æ•°é‡ï¼ˆNï¼‰é»˜è®¤çš„sizeæ•°æ˜¯100ï¼›
    iteræ˜¯æ¨¡å‹è®­ç»ƒæ—¶åœ¨æ•´ä¸ªè®­ç»ƒè¯­æ–™åº“ä¸Šçš„è¿­ä»£æ¬¡æ•°ï¼Œå‡å¦‚å‚ä¸è®­ç»ƒçš„æ–‡æœ¬é‡è¾ƒå°‘ï¼Œå°±éœ€è¦æŠŠè¿™ä¸ªå‚æ•°è°ƒå¤§ä¸€äº›ã€‚iterçš„é»˜è®¤å€¼ä¸º5ï¼›
    sgæ˜¯æ¨¡å‹è®­ç»ƒæ‰€é‡‡ç”¨çš„çš„ç®—æ³•ç±»å‹ï¼š1 ä»£è¡¨ skip-gramï¼Œ0ä»£è¡¨ CBOWï¼Œsgçš„é»˜è®¤å€¼ä¸º0ï¼›
    windowæ§åˆ¶çª—å£ï¼Œå¦‚æœè®¾å¾—è¾ƒå°ï¼Œé‚£ä¹ˆæ¨¡å‹å­¦ä¹ åˆ°çš„æ˜¯è¯æ±‡é—´çš„ç»„åˆæ€§å…³ç³»ï¼ˆè¯æ€§ç›¸å¼‚ï¼‰ï¼›å¦‚æœè®¾ç½®å¾—è¾ƒå¤§ï¼Œä¼šå­¦ä¹ åˆ°è¯æ±‡ä¹‹é—´çš„èšåˆæ€§å…³ç³»ï¼ˆè¯æ€§ç›¸åŒï¼‰ã€‚æ¨¡å‹é»˜è®¤çš„windowæ•°å€¼ä¸º5ï¼›
    """
    #trian
    word_vec = gensim.models.Word2Vec(all_text, vector_size=100,min_count=min_df,window = 5,sg = 0)
    index = word_vec.wv.index_to_key

    #val

    word_vec.save(save_path+'word2vec.bin')
    return index 

def word_cab(data_clear):
    all_text = []
    vocab = {}
    texts = data_clear['Title clear'] + data_clear['News Url clear'] + data_clear['Report Content clear']
            # å°†è¯æ±‡é›†åˆè½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨ï¼Œå¹¶åˆ›å»ºè¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„
    for text in texts:
                #print(i,"\n")
        setence = ''
        if isinstance(text, str):  # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ï¼ˆå‡è®¾æ¯æ®µæ–‡æœ¬æ˜¯å­—ç¬¦ä¸²ï¼‰
            for word in text.split(): # å°†è¯æ±‡åˆ—è¡¨æ·»åŠ åˆ°all_textä¸­
                vocab[word] = vocab.get(word, 0) + 1
                setence = setence + word+' '


        else:
            print(type(text))
            raise ValueError(f"Text at index  is not a string: {text}")
        all_text.append(setence)

    return vocab, all_text
   
def get_data(path):
    """
    æ•°æ®é¢„å¤„ç†ï¼Œå¯¹æ¯ä¸ªæ•°æ®è¡¨çš„æ¯åˆ—è¿›è¡Œå»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œjiebaåˆ†è¯ï¼Œå½¢æˆæ•°æ®1
    ç„¶ååˆé›†è¿™äº›è¯æ±‡ï¼Œå¹¶ä¸”ç»Ÿè®¡è¯æ±‡é¢‘ç‡ï¼Œå½¢æˆè¯é¢‘ç‡è¡¨
    æœ€åè¿”å›ä¸¤ä¸ªcsvçš„pathï¼Œå’Œå…¨æ–‡æœ¬ã€‚
    dataframeåˆ—[id,æ ‡é¢˜ï¼Œæ­£æ–‡ï¼Œå›¾ç‰‡è·¯å¾„ï¼Œè¯„è®ºï¼Œåˆ¤æ–­æ ‡ç­¾ï¼Œclearæ ‡é¢˜ï¼Œclearnæ­£æ–‡ï¼Œclearnè¯„è®º]
    å›¾ç‰‡å¤„ç†æ˜¯ç›´æ¥ä¸ºè·¯å¾„
    """
    os.chdir(path)
    print(os.getcwd())
    # éœ€è¦æ¸…æ´çš„åˆ—è¡¨
    clear_index = ['Title', 'News Url', 'Report Content']
    data_csv = pd.read_csv(path+r'\origin_train.csv', encoding='utf-8')
    # å¤åˆ¶ä¸€ä¸ªå‰¯æœ¬æ–¹ä¾¿åšå¤„ç†é¿å…è¡¨æ ¼è¿‡å¤§
    data_clear = data_csv.copy()
    # å¯¹æŒ‡å®šåˆ—è¿›è¡Œæ¸…æ´—
    for i in clear_index:
        #print(i)
        #print(data_clear[i])
        data_clear[f'{i} clear'] = data_clear[i].apply(lambda x: ssl_clean(x))
        data_clear[f'{i} clear'] = data_clear[f'{i} clear'].apply(lambda x: ' '.join(jieba.cut_for_search(x)))
    #æ‰“ä¸Šæ ‡ç­¾ï¼Œæ˜¯å¦æ­£ç¡®è¯»å–news url
    data_clear['news_tag'] = data_clear['News Url'].apply(check_content_for_errors)
    # æ›´æ”¹image url ä¸ºpath
    images_directory = path+r'\train\image'
    data_clear = update_image_url(data_clear,images_directory)
    #å›¾ç‰‡ç›®å‰æ€ä¹ˆå¤„ç†ä¸çŸ¥é“
    #images = images_process(data_clear['image_path'].tolist())
    #data_clear['image'] = images
    """
    ç›®å‰æš‚å®šè¿™æ ·ç­‰ä¼šï¼Œçœ‹çœ‹å“ªäº›åœ°æ–¹æœ‰é—®é¢˜çš„åœ°æ–¹ç­‰ä¼šå†æ”¹
    """
    # åˆå¹¶è¯æ±‡åšè¯æ±‡è¡¨å’Œå…¨æ–‡æœ¬
    word_to_ix, all_text = word_cab(data_clear)
    print(type(all_text))
    # éšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    val_ratio = 0.3
    total_samples = len(data_clear)
    initial_val_samples = int(total_samples * val_ratio)
    # æ‰“ä¹±ç´¢å¼•
    indices = list(range(data_clear.shape[0]))
    random.shuffle(indices)
    # ç­›é€‰ç¬¦åˆnews_tagæ¡ä»¶çš„æ ·æœ¬ç´¢å¼•
    filtered_indices = [i for i in indices if data_clear.loc[i, 'news_tag'] in [1, 2]]
    val_samples = int(len(filtered_indices) * val_ratio)
    train_samples = len(filtered_indices) - val_samples
    # é€‰æ‹©éªŒè¯é›†è®­ç»ƒé›†æ ·æœ¬
    val_indices = filtered_indices[:val_samples]
    train_indices_initial = filtered_indices[val_samples:]
    half_val_indices = val_indices[:val_samples // 2]
    train_indices_final = train_indices_initial + half_val_indices

    # æ ¹æ®ç´¢å¼•åˆ’åˆ†æ•°æ®é›†
    train_data = data_csv.loc[train_indices_final]
    val_data = data_csv.loc[val_indices[val_samples // 2:]]  # éªŒè¯é›†å‰©ä¸‹çš„ä¸€åŠ

    train_data.to_csv('train.csv', index=False)
    val_data.to_csv('val.csv', index=False)

    return train_data, val_data, word_to_ix, all_text
       
def read_data(text_only,min_df,path = "null"):
    """
    è®­ç»ƒåŠ è½½dataç”¨
    å…ˆè¿›è¡Œè°ƒç”¨get_dataè¿›è¡Œè¯»å–æ–‡ä»¶åˆ†å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¿”å›è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å‚æ•°æƒ…å†µï¼Œä»¥åŠæ–‡ä»¶è·¯å¾„ï¼Œ
    åœ¨è¿™ä¸ªå‡½æ•°å†…åŠ è½½è¿™ä¸ªè¯»å–æ–‡ä»¶ã€‚
    ç„¶åè°ƒç”¨åšåˆ†åˆ«åšä¸¤ä¸ªå…¨æ–‡åˆé›†ä¸¢å…¥è¯å‘é‡å‡½æ•°å¾—åˆ°è¯å‘é‡è¡¨ï¼Œè¿”å›å¹¶ä¸”è¯å‘é‡å‚æ•°æƒ…å†µ

    """
        #text_only = False

    if text_only:
        print("Text only")
        image_list = []
    else:
        print("Text and image")
    print("loding data...")    
    train_data,val_data,word_cab,all_text=get_data(path)

    #è¾“å‡ºå‚æ•°æƒ…å†µ
    print("number of sentences: " + str(len(all_text)))
    print("vocab size: " + str(len(word_cab)))
    max_l = len(max(all_text, key=len))
    print("max sentence length: " + str(max_l))    
    #åŠ è½½è¯å‘é‡
    save_path = r'E:\fakenews\EANN_2024\Data\weixin'
    print("word2vec loaded!")
    index = text_to_word2vec(save_path,all_text)
    return train_data,val_data,all_text,word_cab
   
  
read_data(False,30,path= r'E:\fakenews\EANN_2024\Data\weixin')