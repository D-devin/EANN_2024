import pandas as pd
import jieba
import re
# è¯»å–CSVæ–‡ä»¶
df = pd.read_csv('train/news_train.csv')
#åŠ è½½åœç”¨è¡¨
with open('train/cn_stopwords.txt','r',encoding='utf-8') as f:
 stopwords=f.read().splitlines()
print(len(stopwords))
#æ ‡ç‚¹æ¸…ç†
# æ–‡æœ¬æ¸…ç†å‡½æ•°
def ssl_clean(string):
        # å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¦æ¸…é™¤çš„ç‰¹æ®Šå­—ç¬¦çš„å­—ç¬¦ç±»
        special_chars = u"[ï¼Œã€‚ï¼šï¼›.`|â€œâ€â€”â€”_/&;@ã€ã€Šã€‹ï½ï¼ˆï¼‰#ï¼%ã€ã€‘:,+(ï½œ)ä¸¨â—â–¶â€¦!ï¿½âœ…ãŠ™Òˆâ†‘â†“ğŸ¤£\s*nbsp-]"
        # ä½¿ç”¨re.sub()å‡½æ•°å’Œå­—ç¬¦ç±»æ¥æ¸…é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦
        string = re.sub(special_chars, "", string)
        # å»é™¤å­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦ï¼ˆæ³¨æ„ï¼šè¿™é‡Œä¸å¤„ç†ä¸­é—´çš„ç©ºç™½å­—ç¬¦ï¼‰
        return string.strip()

    #æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
 text = ssl_clean(text)#æ¸…ç†æ ‡ç‚¹
 # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œå¦‚é¡¹ç›®ç¬¦å·â€œâ—â€
 text = re.sub(r'[â—]', '', text)
 text = re.sub(r'[â–¶]', '', text)
 text = re.sub(r'â€¦+', '', text)
 text = re.sub(r'!+', '', text)
 text = re.sub(r'[ï¿½]', '', text)
 text = re.sub(r'[âœ…]', '', text)
 text = re.sub(r'[ãŠ™]', '', text)
 text = re.sub(r'[Òˆ]', '', text)
 text = re.sub(r'[â†‘]', '', text)
 text = re.sub(r'[â†“]', '', text)
 text = re.sub(r'[ğŸ¤£]', '', text)
 text = re.sub(r'[ï»¿]', '', text)
 text=re.sub(r'\s*[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š"â€˜â€™â€œâ€ï¼ˆï¼‰ã€Šã€‹ã€ã€‘\n\t]+\s*', '', text)
 text = re.sub(r'~+', '', text)
 text = re.sub(r'\d+', '', text) #æ¸…ç†æ•°å­—
 words=jieba.cut(text)
 words=[word for word in words if word.strip() and word not in stopwords]
 return ' '.join(words)
#å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
df['News Url']=df['News Url'].apply(preprocess_text)
df['Report Content']=df['Report Content'].apply(preprocess_text)
df['Title']=df['Title'].apply(preprocess_text)
df.to_csv('news_train.csv', index=False)

print('å¤„ç†å®Œæˆ')
#è¯è¡¨åˆ›å»º
#åˆå¹¶
texts = df['Title'] + df['News Url'] + df['Report Content']
# åˆ›å»ºä¸€ä¸ªç©ºçš„åˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰å•è¯
all_words = []
# éå†æ¯æ¡è¯„è®ºï¼Œå¹¶å°†å•è¯æ·»åŠ åˆ°åˆ—è¡¨ä¸­
for text in texts:
    words = text.split()  # å› ä¸ºæˆ‘ä»¬å·²ç»ç”¨ç©ºæ ¼è¿æ¥äº†å•è¯
    all_words.extend(words)

# åˆ›å»ºä¸€ä¸ªè¯å…¸ï¼Œå°†æ¯ä¸ªå•è¯æ˜ å°„åˆ°å”¯ä¸€çš„ç´¢å¼•
# æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Pythonçš„setæ¥å»é™¤é‡å¤çš„å•è¯ï¼Œç„¶åè½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
unique_words = sorted(list(set(all_words)))
word_to_idx = {word: idx for idx, word in enumerate(unique_words)}
idx_to_word = {idx: word for idx, word in enumerate(unique_words)}

# ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
def text_to_idx_sequence(text):
    words = text.split()
    return [word_to_idx[word] for word in words if word in word_to_idx]

from gensim.models import Word2Vec
sentences = df['Title'] + df['News Url'] + df['Report Content'].apply(
    lambda x: x if pd.notnull(x) else '')
sentences = sentences.apply(lambda x: x.split())  # å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯åˆ—è¡¨
# ç¡®ä¿ sentences æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼ˆå¥å­ï¼‰
sentences_list = sentences.tolist()
# æ‰“å¼€æ–‡ä»¶ä»¥å†™å…¥
with open('sentences.txt', 'w', encoding='utf-8') as f:
    for sentence in sentences_list:
        # å°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶ç”¨ç©ºæ ¼åˆ†éš”
        sentence_str = ' '.join(sentence)
        # å†™å…¥å¥å­åˆ°æ–‡ä»¶ï¼Œæ¯ä¸ªå¥å­å ä¸€è¡Œ
        f.write(f'{sentence_str}\n')
# è®­ç»ƒword2Vecæ¨¡å‹
# æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹å‚æ•°ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´å®ƒä»¬ä»¥é€‚åº”æ‚¨çš„æ•°æ®å’Œéœ€æ±‚
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# ä¿å­˜æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
model.save("word2vec.model")

# åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
# model = Word2Vec.load("word2vec.model")

# è·å–æŸä¸ªè¯çš„è¯å‘é‡ï¼ˆä¾‹å¦‚â€œèŒƒå†°å†°â€ï¼‰
# æ³¨æ„ï¼šç”±äºåˆ†è¯å’Œé¢„å¤„ç†çš„åŸå› ï¼Œä½ å¯èƒ½éœ€è¦ç¡®ä¿è¾“å…¥çš„è¯ä¸æ¨¡å‹ä¸­çš„è¯å®Œå…¨åŒ¹é…
word_vector = model.wv['ç¾å¥³'] if 'ç¾å¥³' in model.wv else None
print(word_vector)